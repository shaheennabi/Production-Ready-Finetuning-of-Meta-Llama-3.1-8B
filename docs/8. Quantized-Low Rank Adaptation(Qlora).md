## Quantized Low-Rank Adaptation (QLORA)

Before diving into the **QLORA** technique, let's first understand the concepts involved.

### What is QLORA?

**QLORA** stands for **Quantized Low-Rank Adaptation**. The name itself tells us about the two main components:
1. **Quantization**: The model is first quantized (reduced in precision) from high-precision 32-bit floating point values to a lower precision, such as 4-bit integers. This step reduces the model size significantly, making it more memory-efficient.
2. **Low-Rank Adaptation (LoRA)**: After quantization, we fine-tune the model by adding **low-rank** adaptation layers. These are smaller, trainable matrices that allow the model to adjust to specific tasks without retraining the entire model.

So, in QLORA, we first **quantize the model**, and then apply **LoRA** for fine-tuning.

### What Does Floating Point Mean?

Floating-point numbers can represent a wide range of values, including integers, decimals, and fractions. They are especially useful for representing very large or very small numbers. 

In the case of **32-bit floating-point representation**, the number is broken down into three main components:
- **Sign bit**: 1 bit to indicate whether the number is positive or negative.
- **Exponent**: 8 bits to represent the magnitude of the number (how large or small it is).
- **Mantissa (or significand)**: 23 bits to represent the precision of the number.


#### Example: Converting -3.75 to 32-bit Floating Point (Step by Step)

Let's break down how -3.75 is stored in a computer's memory:

1. **First, write -3.75 in binary:**
   ```
   Step 1: Convert 3 to binary
   3 = 11 (in binary)
   
   Step 2: Convert 0.75 to binary
   0.75 × 2 = 1.5  → Write down 1
   0.5 × 2  = 1.0  → Write down 1
   0.0      → Stop here
   
   So, 0.75 = 0.11 (in binary)
   
   Therefore, -3.75 = -11.11 (in binary)
   ```

2. **Normalize it (move decimal point to get 1.xxx format):**
   ```
   -11.11 = -1.111 × 2¹
   Now we have:
   • Sign: Negative (1)
   • Number: 1.111
   • Power of 2: 1
   ```

3. **Convert to 32-bit format:**

| Part | Explanation | Calculation | Final Bits |
|------|-------------|-------------|------------|
| Sign | Negative = 1, Positive = 0 | Number is negative | 1 |
| Exponent | Power of 2 (1) + Bias (127) | 1 + 127 = 128 = 10000000 | 10000000 |
| Mantissa | Everything after 1. in 1.111 | Just the 111, then pad with zeros | 11100000000000000000000 |

4. **Put it all together:**
   ```
   Sign     Exponent        Mantissa
   1    |   10000000   |   11100000000000000000000
   ```

#### Example: Converting -3.75 to 4-bit (Quantization)

Now, let's see how we convert this same number (-3.75) to a 4-bit number:

1. **Understand the range:**
   ```
   4 bits can represent 16 numbers (0 to 15)
   We need to represent both positive and negative numbers
   So we map:
   • -5.0 → 0
   • 0.0  → 8
   • +5.0 → 15
   ```

2. **Convert -3.75 using a simple proportion:**
   ```
   Original range: -5.0 to +5.0 (total range = 10.0)
   4-bit range: 0 to 15 (16 different values)
   
   Formula:
   New value = 8 + (old value × 8/5)
   For -3.75:
   = 8 + (-3.75 × 8/5)
   = 8 + (-6)
   = 2
   ```

3. **Visual representation of the conversion:**

| Step | Value | Explanation |
|------|-------|-------------|
| Original number | -3.75 | Our starting value |
| Scale factor | 8/5 | How much we multiply to fit in new range |
| Scaled number | -6 | -3.75 × (8/5) |
| Add offset | 2 | 8 + (-6) = 2 |
| Final 4-bit value | 0010 | 2 in binary (4 bits) |


### How Does Quantization Work (32-bit to 4-bit)?

When we **quantize** the model, we convert the floating-point weights from a higher precision (e.g., 32-bit) to a lower precision (e.g., 4-bit). 

#### Example of 32-bit to 4-bit Quantization:

| Original Value (32-bit) | Memory Used | Quantized Value (4-bit) | Memory Used | Possible Values |
|------------------------|-------------|------------------------|-------------|-----------------|
| -3.75 (float32) | 32 bits | 8 (int4) | 4 bits | 16 levels (0-15) |
| 2.5 (float32) | 32 bits | 12 (int4) | 4 bits | 16 levels (0-15) |
| -1.2 (float32) | 32 bits | 4 (int4) | 4 bits | 16 levels (0-15) |

### Quantization Process Example:

Consider a range of weights from -5.0 to 5.0 being quantized to 4-bit (0-15):

| Original Weight | Scaled & Mapped (0-15) | 4-bit Quantized Value |
|----------------|----------------------|---------------------|
| -5.0 | 0 | 0000 |
| -3.75 | 3 | 0011 |
| -2.5 | 6 | 0110 |
| 0.0 | 8 | 1000 |
| 2.5 | 11 | 1011 |
| 3.75 | 13 | 1101 |
| 5.0 | 15 | 1111 |

### Memory Savings Example:

| Model Size | 32-bit Size | 4-bit Size | Memory Savings |
|------------|-------------|------------|----------------|
| 1M params | 4 MB | 0.5 MB | 87.5% |
| 7B params | 28 GB | 3.5 GB | 87.5% |
| 13B params | 52 GB | 6.5 GB | 87.5% |

### Low-Rank Adaptation (LoRA):

Once the model has been quantized to 4-bit precision, we apply **Low-Rank Adaptation (LoRA)** for fine-tuning.

- **LoRA** involves adding small, trainable low-rank matrices to the model. These matrices are not part of the original model but allow fine-tuning without retraining the full model.
- This reduces the computational cost and memory footprint, as only the small low-rank matrices are trained.

In practice, **QLORA** applies LoRA on top of the quantized model to enable efficient fine-tuning on tasks with limited computational resources.

---

### Summary of QLORA:

- **QLORA = Quantization + LoRA**:  
  1. **Quantization**: The model's weights are converted from 32-bit floating-point precision to lower precision (like 4-bit integers) to reduce memory usage.
  2. **LoRA**: Small, trainable matrices are added for fine-tuning the model on specific tasks without retraining the entire model.
  
- **Quantization Process**:
  1. Convert from high precision (32-bit) to low precision (e.g., 4-bit) by scaling and clipping.
  2. The weights are stored as integers, reducing memory usage.
  
- **LoRA Fine-tuning**:  
  Only small low-rank matrices are trained, saving time and resources.

This combination of **quantization** and **LoRA** makes QLORA a powerful technique for fine-tuning large models like LLaMA 8B on machines with limited memory and computational power.