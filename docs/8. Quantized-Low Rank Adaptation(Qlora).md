## Quantized Low-Rank Adaptation (QLORA)

Before diving into the **QLORA** technique, let's first understand the concepts involved.

### What is QLORA?

**QLORA** stands for **Quantized Low-Rank Adaptation**. The name itself tells us about the two main components:
1. **Quantization**: The model is first quantized (reduced in precision) from high-precision 32-bit floating point values to a lower precision, such as 4-bit integers. This step reduces the model size significantly, making it more memory-efficient.
2. **Low-Rank Adaptation (LoRA)**: After quantization, we fine-tune the model by adding **low-rank** adaptation layers. These are smaller, trainable matrices that allow the model to adjust to specific tasks without retraining the entire model.

So, in QLORA, we first **quantize the model**, and then apply **LoRA** for fine-tuning.

### What Does Floating Point Mean?

Floating-point numbers can represent a wide range of values, including integers, decimals, and fractions. They are especially useful for representing very large or very small numbers. 

In the case of **32-bit floating-point representation**, the number is broken down into three main components:
- **Sign bit**: 1 bit to indicate whether the number is positive or negative.
- **Exponent**: 8 bits to represent the magnitude of the number (how large or small it is).
- **Mantissa (or significand)**: 23 bits to represent the precision of the number.

#### Example of 32-bit Floating Point Representation:

Consider the number **-3.75**.

1. **Sign bit (1 bit)**: Since the number is negative, the sign bit will be **1**.
2. **Exponent (8 bits)**: The exponent is used to scale the number. In binary, **3.75** is represented as **1.111** (in binary) Ã— **2^1**. The exponent in this case will represent **1** (after bias adjustment).
3. **Mantissa (23 bits)**: The mantissa represents the fractional part of the number. For **1.111** in binary, we store just the fractional part **111000...** up to 23 bits.

So, **-3.75** in 32-bit floating point is represented as:

Sign bit: 1 Exponent: 10000000 (binary for 1, with bias added) Mantissa: 11100000000000000000000 (the precision part)


### How Does Quantization Work (32-bit to 4-bit)?

When we **quantize** the model, we convert the floating-point weights from a higher precision (e.g., 32-bit) to a lower precision (e.g., 4-bit). 

#### Converting from 32-bit to 4-bit Precision:

- **Before Quantization (32-bit)**:  
  The original model weights are represented in 32-bit precision, using 1 bit for sign, 8 bits for exponent, and 23 bits for the mantissa.
  
  Example: 
  - A weight of `-3.75` would be represented as a 32-bit floating-point number, taking up 4 bytes.

- **After Quantization (4-bit)**:  
  The weight is scaled to fit within the range that a 4-bit integer can represent, i.e., **16 possible values** (from 0 to 15).
  
  Example:
  - After scaling, **-3.75** might be quantized to an integer, say **8**, which fits into the 4-bit integer range [0, 15].

### Steps in the Quantization Process: (maybe using bitsandbytes library, we use)

1. **Scale the Values**:  
   The range of the weights (e.g., from -5.0 to 5.0) is scaled to fit within the range of the target precision (e.g., 0 to 15 for 4-bit precision).
   
   Example:  
   If the weight range is from **-5.0 to 5.0**, we map these values to **0 to 15** using a linear scale.
   
2. **Clip the Values**:  
   If any values go out of bounds (outside the range [0, 15] for 4-bit), they are clipped to the nearest valid value.

3. **Convert to Integer**:  
   The scaled and clipped values are then converted to integers. For example:
   - If `-3.75` scales to `8` in the 4-bit range, it is stored as `8` instead of a floating-point number.

### Low-Rank Adaptation (LoRA):

Once the model has been quantized to 4-bit precision, we apply **Low-Rank Adaptation (LoRA)** for fine-tuning.

- **LoRA** involves adding small, trainable low-rank matrices to the model. These matrices are not part of the original model but allow fine-tuning without retraining the full model.
- This reduces the computational cost and memory footprint, as only the small low-rank matrices are trained.

In practice, **QLORA** applies LoRA on top of the quantized model to enable efficient fine-tuning on tasks with limited computational resources.

---

### Summary of QLORA:

- **QLORA = Quantization + LoRA**:  
  1. **Quantization**: The model's weights are converted from 32-bit floating-point precision to lower precision (like 4-bit integers) to reduce memory usage.
  2. **LoRA**: Small, trainable matrices are added for fine-tuning the model on specific tasks without retraining the entire model.
  
- **Quantization Process**:
  1. Convert from high precision (32-bit) to low precision (e.g., 4-bit) by scaling and clipping.
  2. The weights are stored as integers, reducing memory usage.
  
- **LoRA Fine-tuning**:  
  Only small low-rank matrices are trained, saving time and resources.

This combination of **quantization** and **LoRA** makes QLORA a powerful technique for fine-tuning large models like LLaMA 8B on machines with limited memory and computational power.

This document is written in simple terms to help readers understand **LORA**. The content is a mix of original writing and contributions from ChatGPT for better clarity.