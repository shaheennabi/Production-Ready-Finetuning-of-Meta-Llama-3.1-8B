## Working of LORA
# In-Depth Explanation of LoRA (Low-Rank Adaptation) and Its Key Concepts

LoRA (Low-Rank Adaptation) is a technique designed to efficiently fine-tune large pre-trained models, like transformer-based models, without modifying the original weight matrix \( W \). Instead of modifying the original weights, LoRA introduces additional parameters that represent **low-rank updates**, allowing efficient training while preserving the integrity of the pre-trained model. This is done through the introduction of two smaller matrices, \( A \) and \( B \), whose product approximates the update to the original weight matrix.

Let’s break down the key concepts, with a focus on the **rank \( r \)**, and understand the terminology and why it is important.

---

## 1. The Original Weight Matrix \( W \)

In any neural network, especially in transformer-based models, the weight matrix \( W \) connects different layers of the model, and it is responsible for transformations between input and output features.

- **Shape of \( W \)**: \( W \) typically has dimensions \( d_{\text{out}} \times d_{\text{in}} \), where:
  - \( d_{\text{out}} \) is the output dimension (number of neurons in the layer).
  - \( d_{\text{in}} \) is the input dimension (number of features from the previous layer).

Fine-tuning a large model normally involves updating this large weight matrix \( W \), which can be computationally expensive and prone to overfitting, especially for massive models.

---

## 2. LoRA: The Core Idea

LoRA addresses the problem of fine-tuning large models by introducing a **low-rank approximation** to the update of \( W \), instead of directly modifying \( W \). The idea is to keep the pre-trained model frozen and instead learn only a small number of additional parameters that represent the update to the weights.

- **Low-rank approximation**: Rather than updating \( W \) directly, LoRA approximates the change to \( W \) using two matrices, \( A \) and \( B \), such that:

$$
W_{\text{new}} = W + AB^T
$$

where:
  - \( A \) has shape \( d_{\text{out}} \times r \),
  - \( B \) has shape \( d_{\text{in}} \times r \),
  - \( r \) is a small rank, typically a small number like 4 or 8.

This means that LoRA is **injecting two smaller matrices** (not modifying \( W \) itself) and learning only these small matrices.

---

## 3. The Role of the Rank \( r \)

### a. What is Rank \( r \)?
- **Rank** of a matrix refers to the number of **linearly independent rows or columns** it has.
  - A matrix of rank \( r \) can be expressed as a product of \( r \)-dimensional vectors (rows or columns), which implies that the matrix is only capturing relationships in an \( r \)-dimensional space, instead of capturing all possible relationships in the original, high-dimensional space.
  
- In LoRA, \( r \) represents the **dimensionality** of the update we are injecting into the model. It controls how much of the weight update can be "compressed" into a low-dimensional space.

- For example, if \( r = 4 \), the update to \( W \) is constrained to 4 independent directions, even if the original weight matrix \( W \) is much larger (i.e., has much higher dimensionality).

### b. Why Is Rank \( r \) Important?
- **Efficiency**: The choice of \( r \) directly influences the number of parameters you are adding to the model. With smaller \( r \), the number of parameters remains small, allowing for more efficient training. 
  - The number of parameters added is \( d_{\text{out}} \cdot r + d_{\text{in}} \cdot r \), which is much smaller compared to \( d_{\text{out}} \cdot d_{\text{in}} \) (the number of parameters in the full weight matrix \( W \)).
  
- **Control Over Complexity**: A small \( r \) constrains the update to be **low-dimensional**. This is beneficial because you only want the model to adapt to the specific task at hand, without overfitting or making large, unnecessary changes to the pre-trained knowledge encoded in \( W \).
  - This prevents overfitting and ensures that only **task-specific** changes are learned.

- **Memory and Computational Efficiency**: If \( r \) is small, LoRA only needs to store and compute the updates represented by \( A \) and \( B \), rather than the full weight matrix \( W \). This greatly reduces both memory usage and computational cost.

### c. How to Choose Rank \( r \)?
- \( r \) is a hyperparameter, and its choice depends on the task and the model:
  - Smaller \( r \) (e.g., 4 or 8) is often enough for most tasks, especially if the task is not very complex.
  - Larger \( r \) may be needed for tasks that require more capacity for the weight update, but this increases memory and computation.

---

## 4. The Matrices \( A \) and \( B \):

- **Matrix \( A \)**: It has dimensions \( d_{\text{out}} \times r \), which means it learns \( r \) linear combinations of the output dimension. 
- **Matrix \( B \)**: It has dimensions \( d_{\text{in}} \times r \), learning how to project the input dimension into the \( r \)-dimensional space.

- The product \( AB^T \) gives the low-rank update matrix:

$$
AB^T \quad \text{of size} \quad d_{\text{out}} \times d_{\text{in}}
$$

- This matrix \( AB^T \) is added to the original weight matrix \( W \), which results in:

$$
W_{\text{new}} = W + AB^T
$$

This update is what allows the model to adapt to new data or tasks, without changing the core pre-trained model.

---

## 5. Why Does LoRA Use \( A \) and \( B \) (and Not Just One Matrix)?

- **Why two matrices**? Using two matrices, \( A \) and \( B \), enables LoRA to break the update into two parts:
  - The matrix \( A \) learns to represent the change in the output space.
  - The matrix \( B \) learns how to project the change into the input space.
  
- The combination of these two smaller matrices allows for a **more efficient representation** of the update to \( W \) than learning a single large matrix directly. 

- The **rank \( r \)** defines the bottleneck through which this update must pass, limiting the number of degrees of freedom and enforcing a **low-rank constraint**.

---

## 6. End-to-End Parameter Update Process

Here’s how the process works from a high level:

1. **Initialization**: The original pre-trained weight matrix \( W \) is frozen. LoRA introduces two new matrices, \( A \) and \( B \), with small random values, and \( r \) is set (e.g., 4).
2. **Forward Pass**: During the forward pass, the model computes the output by using \( W \) along with the low-rank updates \( AB^T \). So, the output is:

$$
y = W_{\text{new}} \cdot x = (W + AB^T) \cdot x
$$

The original \( W \) remains unchanged, and the change comes from the product \( AB^T \).
3. **Backpropagation**: During backpropagation, gradients are computed and used to update \( A \) and \( B \). Since \( W \) is frozen, only \( A \) and \( B \) are updated. This is the key aspect of LoRA — **the original model parameters are not modified**, only the low-rank update parameters \( A \) and \( B \) are.
4. **Final Model**: Once training is complete, the new weight matrix is:

$$
W_{\text{new}} = W + AB^T
$$

where \( AB^T \) represents the task-specific adjustment. This allows you to use the pre-trained model for the new task without modifying its original weights.

---

## 7. Conclusion:

- **Rank \( r \)** is a fundamental part of LoRA because it defines the **dimensionality of the low-rank update** and controls the efficiency, expressiveness, and memory usage of the adaptation.
- The matrices \( A \) and \( B \) serve to approximate the changes to the original weight matrix \( W \), with \( r \) being the key hyperparameter that constrains the complexity of these updates.
- **Choosing \( r \)** is a trade-off between computational efficiency and the model’s ability to express the necessary task-specific changes.

The main advantage of LoRA is that it allows efficient fine-tuning of large pre-trained models without the need to modify the large weight matrices directly. Instead, you introduce a low-rank update that is computationally efficient and easy to learn.


This document is written in simple terms to help readers understand **LORA**. The content is a mix of original writing and contributions from ChatGPT for better clarity.