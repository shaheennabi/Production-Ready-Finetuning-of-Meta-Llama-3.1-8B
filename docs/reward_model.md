## Reward Model Working

![Reinforcement-Learning-1](https://github.com/user-attachments/assets/be55dc2c-02b4-4779-b5e4-7d5e0a00ca45)

Before diving deep into this project, it's crucial to understand how **Large Language Models (LLMs)** are trained. In this file, we will focus on the **reward model**.

As shown in the image, when we prompt a large language model like ChatGPT (based on GPT-4 or similar models), we get a response. For example:

**Prompt:** I like to play cricket  
**Response 1:** Ok! It's good to know that you like cricket.

When ChatGPT was initially launched (November-December 2022), it performed well but occasionally produced suboptimal responses. For example, **Response 1** above is okay but not great. A better response might be:

**Response 2:** Wow! Cricket is a great game as it improves your health and your mental ability.

Do you notice the difference? **Response 2** is significantly more engaging and human-aligned than **Response 1**. This challenge—to make LLM responses more human-preferred—gave rise to the **Reward Model**.

The **Reward Model** is distinct from the main LLM (e.g., GPT model). Its job is to assign a score (reward signal) to responses generated by the main LLM. Let’s delve deeper.

---

## Working of Reward Model

The reward model is a **separate model**, trained independently on responses from the main LLM.

For instance:  
**Prompt:** Hi, how are you?

Generated responses:  
- **Response 1:** I am good.  
- **Response 2:** Hi there, I am doing great!  
- **Response 3:** Thanks for asking, I am doing great. How have you been?  

Here, **Response 3** is the most human-aligned and engaging, followed by **Response 2** and then **Response 1**. Rankings might look like this:  
**Response 3 > Response 2 > Response 1**

This ranking system is how the reward model works internally. Responses from the LLM are collected and ranked by human labelers (researchers). These rankings are then converted into scores. For example:  
- **Response 3** = 0.95  
- **Response 2** = 0.7  
- **Response 1** = 0.4  

The goal is to make the main LLM generate responses like **Response 3**. To achieve this, the prompt-response pairs are used as input, and the score becomes the target output for training the reward model.

---

### Training the Reward Model

The reward model is trained using a transformer-based architecture, similar to how deep neural networks are trained.  

**Inputs:**  
- **Prompt:** Hi, how are you?  
- **Response:** Thanks for asking, I am doing great. How have you been?  

**Output (Target):**  
- **Score:** 0.95  

The reward model is trained on such input-output pairs. When given a prompt and response, the trained reward model outputs a **scalar score**.

---

## Summary

The reward model assigns a score (reward signal) to responses generated by the main LLM. It is trained like any deep neural network, using prompt-response pairs as input and scores as output. Once trained, the reward model guides the main LLM to produce more human-aligned responses.

**Up Next:** In the next file, *RLHF_with_PPO.md*, we will explore how this reward model is used to fine-tune the main LLM.


Caution at last: I wrote this and asked ChatGPT to improve it. Not fully generated by language model. From my side it will be easy to understand(hope you got it).