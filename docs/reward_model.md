## Reward Model Working

![Reinforcement-Learning-1](https://github.com/user-attachments/assets/be55dc2c-02b4-4779-b5e4-7d5e0a00ca45)

So, before going deep inside this project, I will explain some important concepts regarding how **Large Language Models** are actually trained. In this file, I will focus on the **reward model**.

As you can see in the image, when we prompt a large language model like ChatGPT based on GPT-4 or other models running in the backend, we get a response. For example, I might ask ChatGPT: *I like to play cricket*, and it responds with: 

(Response 1): *Ok! It's good to know that you like cricket.*

Now, observe here: when ChatGPT was initially launched in November-December 2022, it was good at many things but sometimes gave misleading or suboptimal answers. For instance, in the example above, the response *Ok! It's good to know that you like cricket* might not be the best response. A better response could be:

(Response 2): *Wow! Cricket is a great game as it improves your health and your mental ability.*

Can you see the difference between these responses? Response 2 is far better than Response 1. This posed a challenge for researchers: ensuring LLMs generate responses that are more human-preferred (aligned with human expectations). This is where the **Reward Model** comes into play. 

The reward model is distinct from the main LLM (e.g., any GPT model). What it does is assign a score (a.k.a. reward signal) to the responses generated by the main LLM. Let me describe this briefly:

## Working of Reward Model

The reward model is a **separate model**, trained independently on the responses generated by the main LLM (e.g., GPT).

For example: 
Let's ask ChatGPT: 

**Prompt:** Hi, how are you?

**Response 1:** I am good.

And a few more responses:

**Response 2:** Hi there, I am doing great!

**Response 3:** Thanks for asking, I am doing great. How have you been?

Now, observe these responses. We can see that **Response 3** is far better and more human-aligned than **Response 2** and **Response 1**. We can assign rankings like this: **Response 3 > Response 2 > Response 1**.

This is exactly how the reward model works internally. When the main LLM generates responses, these responses are collected, and human labelers (in this case, researchers) rank them based on how well they align with human preferences (i.e., what humans expect).

As seen in the figure, the responses from the LLM are labeled by human annotators, who rank them. For each **response**, a score is assigned. For example:

- **Response 3** = 0.95  
- **Response 2** = 0.7  
- **Response 1** = 0.4  

We want the main LLM to generate responses like **Response 3**. So, we pair the prompt and response as input and use the score as the output to train the reward model.

### Training the Reward Model

The reward model is trained using a transformer-based architecture, similar to how we train deep neural networks. The prompt and response serve as **independent variables**, and the **score** is the dependent variable (the target the model has to predict).

For example:  
Input:  
- **Prompt:** Hi, how are you?  
- **Response:** Thanks for asking, I am doing great. How have you been?  
- **Score:** 0.95  

The reward model is trained on this input-output pair. When given a prompt and response, it outputs a **scalar score**.

### Summary

The reward model is trained like any deep neural network and provides a scalar output (**score**). Once trained, the reward model is used to fine-tune the main LLM (e.g., ChatGPT or GPT model). 

**In the next file named: RLHF_with_PPO, I will explain how this Reward Model is used to fine-tune or guide the main LLM.**
