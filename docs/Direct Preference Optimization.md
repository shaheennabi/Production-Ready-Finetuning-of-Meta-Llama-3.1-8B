# **Direct Preference Optimization**

In this section, we explore **Direct Preference Optimization (DPO)**, a technique for fine-tuning large language models (LLMs), such as GPT, with human feedback.

Unlike **RLHF with PPO**, **DPO** does not require separate training of a reward model. Instead, it directly leverages human preferences to optimize the LLM's outputs. While some steps overlap with RLHF, the way the loss function updates the model parameters is distinct.

---

## **Working of Direct Preference Optimization**

### **Step 1: Generate Responses**
1. The LLM is given an input query and generates two responses, **A** and **B**.
2. Human feedback is provided in the form of a binary preference:
   - **Preference(A, B) = 1**: if **A** is better.
   - **Preference(A, B) = 0**: if **B** is better.

---

### **Step 2: Score Calculation**
- The model assigns **scores (logits)** to both responses.
- These scores represent how likely the model thinks each response is the correct one.

---

### **Step 3: Softmax Normalization**
- The logits are passed through a **softmax function**, which converts them into probabilities:

$$
P(A|Q) = \frac{\exp(S(A|Q))}{\exp(S(A|Q)) + \exp(S(B|Q))}
$$

$$
P(B|Q) = \frac{\exp(S(B|Q))}{\exp(S(A|Q)) + \exp(S(B|Q))}
$$

- These normalized probabilities represent the model's confidence in each response.

---

### **Step 4: Log-Likelihood Ratio**
- The **log-likelihood ratio** is computed from the probabilities:

$$
\text{LLR} = \log(P(A|Q)) - \log(P(B|Q))
$$

- This measures the relative likelihood of the preferred response over the non-preferred one.

---

### **Step 5: Penalty Term**
- If the probabilities of both responses are **similar**, it indicates that the model is **not confident enough** in distinguishing between the two responses.
- A **penalty term** (e.g., KL Divergence) is applied to encourage the model to favor the preferred response more strongly.

---

### **Step 6: Loss Calculation**
- The **Binary Cross-Entropy (BCE)** loss is used to calculate the final loss:

$$
\mathcal{L}_{\text{BCE}} = - \left[ y \cdot \log(P(A|Q)) + (1-y) \cdot \log(P(B|Q)) \right]
$$

  - **Penalty Integration**: The **penalty term** is combined with the **log-likelihood** within this loss function.
  - The BCE loss includes:
    1. A term to encourage the model to assign higher probabilities to the **preferred response**.
    2. A penalty to **penalize insufficient confidence** in favoring the correct response.

---

### **Step 7: Parameter Update**
- The **loss** is backpropagated through the model to compute **gradients** of the loss with respect to the model parameters.
- Using **gradient descent**, the model parameters are updated to reduce the loss, improving the model's ability to align with human preferences.

---

### **Key Highlights**
1. **No Separate Reward Model**: Unlike RLHF, **DPO** directly integrates human preferences, simplifying the process.
2. **Softmax Normalization**: Converts logits to probabilities for better interpretability.
3. **Log-Likelihood + Penalty**: BCE loss combines **log-likelihood** and **confidence penalty** to ensure the model favors correct responses confidently.
4. **End Goal**: Iteratively improves the model to better align with human feedback by minimizing the loss.

---

Caution at last: I wrote this and asked ChatGPT to improve it. Not fully generated by language model. From my side it will be easy to understand(hope you got it).
