## Direct Preference Optimization

Ok, now let's talk about **Direct Preference Optimization**. In the previous files, we saw how we fine-tune the main LLM (e.g., GPT model) with the help of **Proximal Policy Optimization** guided by the reward model.

But in the case of **Direct Preference Optimization**, the approach is different. **DPO** benefits from the reward model because we don't have to train the reward model separately, as we do in **RLHF with PPO**. **DPO** directly optimizes the responses generated by the main LLM (e.g., GPT model). Some steps are similar to those used in **RLHF with PPO**, but the way the loss function updates the parameters is different.

---

## Working of Direct Preference Optimization