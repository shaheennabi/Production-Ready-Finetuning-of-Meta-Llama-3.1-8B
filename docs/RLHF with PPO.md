# **Reinforcement Learning from Human Feedback (RLHF) with PPO**

What is it, and why does it seem so complex? In this guide, I will take you through the concept of **RLHF** with **Proximal Policy Optimization (PPO)** to demystify its workings.

---

## **Introduction to RLHF with PPO**

Reinforcement Learning became a hot topic in 2016â€“2018 when Google DeepMind released AlphaGo, showcasing the potential of this technology. However, RLHF takes it a step further by aligning language models like ChatGPT with human preferences.

So, how does **PPO** fit into this? Think of it as an **optimization algorithm** in the field of Reinforcement Learning, similar to how optimizers like Adam or SGD improve neural networks. PPO specifically helps fine-tune the LLM using feedback from a reward model.

---

## **Working of RLHF with PPO**

As discussed in **reward_model.md**, the reward model scores responses generated by the main LLM (e.g., GPT) based on human preferences. The reward signal is then used to guide the LLM through fine-tuning.

### **Step-by-Step Process**

1. **Response Generation**:  
   The main LLM generates responses to a given prompt.  
   Example:
   - **Prompt**: I like cricket  
   - **Response (Before RLHF)**: Cricket was introduced in India by the British.

2. **Reward Scoring**:  
   The reward model scores the response based on how well it aligns with human preferences.  
   Example: Score = 0.95.

3. **Fine-Tuning with PPO**:  
   If the response is not aligned well, the reward signal guides the LLM's fine-tuning:
   - The LLM is treated as an **agent**.
   - PPO optimizes the agent's **policy** (response-generation mechanism) using the reward signal and an **advantage function**.

   The policy update in PPO is given by the formula:

   $$ 
   L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right] 
   $$

   Where:
   - \( r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)} \)
   - \( \hat{A}_t \) is the advantage function.
   - \( \epsilon \) is a hyperparameter that controls the degree of clipping to prevent large policy updates.

4. **Updated Response (After RLHF)**:  
   - **Prompt**: I like cricket  
   - **Response**: Wow, cricket is a great game! Glad you like a sport.

---

## **Policy Network Architecture in RLHF**

During the **RLHF** process, the **policy network** of the LLM undergoes fine-tuning. The **policy network** refers to the architecture that the LLM uses to generate responses to given prompts. This architecture is modified and adjusted during the fine-tuning phase to align better with the human preferences as indicated by the reward model.

### **1. LLM as an Agent**

In the RL framework, the LLM is treated as an **agent** that learns from the environment (human feedback) to improve its policy (the way it generates responses). The policy network is the component responsible for generating a distribution over the possible responses given the input prompt.

### **2. Updates to the Policy Network**

PPO updates the policy network by using the reward signal to adjust the parameters of the model. The parameters of the LLM's architecture (such as weights and biases in the neural network layers) are updated using gradients computed from the reward model's feedback.

   The update rule for PPO's policy network is designed to prevent large updates that could harm the model's performance. The clipped objective function ensures that the changes to the policy are gradual and stable. This process effectively "fine-tunes" the LLM to generate more human-aligned responses.

### **3. Reward Signal and Policy Improvement**

The reward signal serves as a feedback mechanism that directly influences how the policy network evolves. If the LLM generates a response that is closer to human preferences, the reward signal will be higher, reinforcing the policy towards generating similar responses in the future.

If the response is far from what is desired, the reward signal will be low, and the policy network will be adjusted accordingly, guiding the model to generate responses that better align with human preferences.

---

## **Key Concepts in RLHF with PPO**

### **1. Reinforcement Learning Framework**

- **Agent**: The LLM, generating responses.
- **Environment**: The feedback loop, including prompts and human preferences.
- **Reward Signal**: Guides the agent to improve its policy.

---

### **2. Over-Optimization and Frozen LLM**

Over-optimization occurs when the LLM becomes overly biased toward the reward model's scores, compromising its general-purpose abilities. 

To prevent this, a **frozen LLM** (a copy of the original model) is used for comparison:
- The frozen LLM ensures the updated model doesn't deviate too far from its original capabilities.
- The reward model's preferences are balanced against the original LLM's responses.

#### **Example of Over-Optimization**

- **Prompt**: What are the benefits of exercise?  
  - **Frozen LLM**: Exercise improves health and mood.  
  - **Updated Model**: Exercise is the ultimate key to living forever. Everyone should exercise daily.

The frozen LLM curbs such unrealistic outputs, ensuring balance.

---

### **3. KL Loss (Kullback-Leibler Divergence)**

**KL Loss** measures how the updated model's response probabilities deviate from the frozen LLM's probabilities:

$$
D_{KL}(P || Q) = \sum P(x) \log \frac{P(x)}{Q(x)}
$$

Where:
- \(P(x)\): Probability distribution of the updated model.
- \(Q(x)\): Probability distribution of the frozen model.

- **Purpose**:  
  - Penalizes large deviations from the frozen LLM.  
  - Ensures the updated model retains its general-purpose functionality.  

---

## **Summary**

1. **RLHF** with **PPO** is a powerful optimization technique that fine-tunes LLMs to align with human preferences.
2. **Reward Model**: Scores responses to guide fine-tuning.  
3. **PPO Algorithm**: Updates the LLM's policy using the reward signal.  
4. **Frozen LLM**: Prevents over-optimization and maintains general-purpose capabilities.  
5. **KL Loss**: Ensures the updated model balances alignment with human feedback and general functionality.
6. **Policy Network**: The LLM's architecture that generates responses is fine-tuned through PPO using the reward signal.

### **Takeaway**

RLHF is like training the LLM with feedback loops to guide its behavior, making it more human-aligned. While **PPO** is a popular optimization algorithm used here, research is rapidly evolving with new methods emerging.

---

_Caution_: This document was written by me, with some improvements from ChatGPT. The concepts are presented in an easy-to-understand manner with the hope that you find them useful.
