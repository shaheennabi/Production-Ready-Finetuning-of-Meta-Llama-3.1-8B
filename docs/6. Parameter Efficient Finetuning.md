## Why PEFT

First of all, do you know that when ChatGPT came, it was trained on the GPT-3 model, which had 175 billion parameters? Now, how can you say anyone can fine-tune such a large model without any higher compute resources? **PEFT** is mostly to finetune certain layers of the base architecture while keeping main base model frozen and more ..

This is where **Parameter Efficient Fine-Tuning (PEFT)** comes in. In PEFT, we reduce the cost of compute, memory, etc.

**Note:** When LLMs or base models come, they are typically in 32-bit or 16-bit format. So, having such a huge parameter size and later fine-tuning them is a big challenge because we need to convert that `float32` to a lower precision like `int8` to reduce compute and memory usage. Here's how it's done:

## Quantization Topics (it will help understand QLORA later)
## How 32-bit Looks

### What is a Float32 Representation?

A 32-bit floating-point number (`float32`) has the following structure:

- **Sign bit (1 bit):** Indicates positive (0) or negative (1).
- **Exponent (8 bits):** Represents the range of the number.
- **Mantissa (23 bits):** Represents the precision of the number.

Example: Let's say the `float32` number is 5.75. Its binary structure would look like:

- **Sign:** 0 (positive number)
- **Exponent:** 2 (bias = 127 → stored value = 129)
- **Mantissa:** 1.1100... (fractional part in binary)

So, 5.75 (float32) is represented as:

1.110 × 2^2

### Step 2: Quantizing Float32 to Int8

**Goal:** Convert a `float32` model (32 bits per parameter) to an `int8` model (8 bits per parameter) while preserving the model's performance as much as possible.

#### Example: From Float32 to Int8

**Step 1: Starting Float32 Values**

Suppose the model has the following `float32` weights for a layer:

Weights: [4.5, -2.3, 0.0, 5.9, -6.2]

**Step 2: Determine the Float32 Range**

Find the minimum and maximum values in the weights:
Range of Float32 Weights: [-6.2, 5.9]

**Step 3: Decide the Int8 Range**

Int8 is a signed 8-bit integer with the range:

Range of Int8: [-128, 127]

**Step 4: Calculate the Scaling Factor**

The scaling factor is the ratio of the Int8 range to the Float32 range:

Scale Factor = (127 - (-128)) / (5.9 - (-6.2)) ≈ 21.07

**Step 5: Scale the Float32 Values**

Multiply each `float32` weight by the scale factor:

Scaled Value = Float Value × Scale Factor

Let's calculate for each weight:

- `4.5 × 21.07 = 94.815`
- `-2.3 × 21.07 = -48.461`
- `0.0 × 21.07 = 0.0`
- `5.9 × 21.07 = 124.313`
- `-6.2 × 21.07 = -130.634`

**Step 6: Round to the Nearest Integer**

Round the scaled values to the nearest integer:

- `round(94.815) = 95`
- `round(-48.461) = -48`
- `round(0.0) = 0`
- `round(124.313) = 124`
- `round(-130.634) = -131`

**Step 7: Clamp to Int8 Range**

Ensure the rounded values fit into the Int8 range `[-128, 127]`:

- `95` is within range.
- `-48` is within range.
- `0` is within range.
- `124` is within range.
- `-131` is out of range → clamp it to `-128`.

The final `int8` values are:

Quantized Weights: [95, -48, 0, 124, -128]

**Step 8: Store the Scale Factor**

The scale factor `≈ 21.07` is stored as metadata to allow dequantization during inference.

**Step 9: Dequantization (During Inference)**

To reconstruct the `float32` values during inference, multiply the `int8` weights by the inverse of the scale factor:

Float Value = Quantized Value / Scale Factor

Let's dequantize the values:

- `95 / 21.07 ≈ 4.51`
- `-48 / 21.07 ≈ -2.28`
- `0 / 21.07 = 0.0`
- `124 / 21.07 ≈ 5.88`
- `-128 / 21.07 ≈ -6.07`

The dequantized `float32` values are:

[4.51, -2.28, 0.0, 5.88, -6.07]


So now, you learned how float32 is converted to Int8
We will use this converting Float32 to Int8 in **QLORA**, make sure you understand **Quantization first**

# PEFT and How It Works

## What is PEFT?

PEFT (Parameter Efficient Fine-Tuning) is a collection of techniques that allow you to fine-tune large language models while only training a small subset of parameters. Instead of updating all model weights during training, PEFT methods introduce a small number of trainable parameters while keeping most of the original model frozen.

*Note: The working of **PEFT** described here is without LORA and QLORA (as these are methods of PEFT, which will be covered in later documents).*

## Working of PEFT

### 1. Start with a Pre-trained Model

The process begins with a **pre-trained model** (e.g., BERT, GPT), which has already been trained on millions or even billions of parameters. These models learn general features from a vast amount of data. The goal here is to preserve the pre-trained knowledge while allowing the model to specialize in a new task by introducing **smaller adapter layers**.

- **Pre-trained Model**: Trained on a large corpus, it already contains useful general knowledge.
- **Task Specialization**: New tasks can be tackled by introducing small task-specific layers, reducing computational cost and time.

### 2. Insertion of Adapter Layers

Adapter layers are a powerful and flexible method for implementing PEFT. These are small, task-specific layers added to a pre-trained model. This method reduces computational cost and training time by only requiring updates to a small number of parameters.

- **Adapter Layers**: Inserted between the original layers of the pre-trained model, typically after the attention layers or feed-forward networks in the transformer architecture.
- **Location in Model**: They are inserted in specific parts of the model (often after the attention mechanism) to modify the model's behavior for task-specific outputs.

### 3. Fine-tuning Adapter Layers

During the fine-tuning phase, only the adapter layers are trained. The weights of the pre-trained model are kept frozen, and only the adapter layers' parameters are updated.

- **Frozen Pre-trained Weights**: The core model's parameters are not updated during fine-tuning.
- **Task-Specific Training**: The adapter layers learn the features specific to the new task based on the provided data.

### 4. Freezing the Pre-trained Model

Once the adapter layers are inserted, the next step is to freeze the original model’s parameters, ensuring that only the adapter layers are trained during fine-tuning.

- **Frozen Parameters**: The weights of the pre-trained model (e.g., attention, embeddings) are frozen, and their gradients are not calculated during training.
- **Trainable Adapter Layers**: Only the small adapter layers are updated during the fine-tuning process, reducing the computational load.

### 5. Fine-tuning the Model

After inserting the adapter layers, you can proceed to fine-tune the model with task-specific data.

- **Load Task-Specific Data**: Prepare the dataset for the specific task (e.g., text classification, named entity recognition).
- **Train Adapter Layers**: Train the model by updating only the parameters of the adapter layers, while the pre-trained layers remain unchanged.

---

This is a streamlined approach to PEFT using adapter layers, which allows the model to specialize for new tasks while keeping most of the original model intact and computationally efficient.


Remember: This is the basic idea or working of **Parameter Efficient Finetuning**, and today we use the popular methods like **LORA(Low Rank Adaptation and QLORA(Quantized Low Rank Adaptation))**, and those are discussed in next documenents


## Caution

This document is written in simple terms to help readers understand **PEFT**. The content is a mix of original writing and contributions from ChatGPT for better clarity.

