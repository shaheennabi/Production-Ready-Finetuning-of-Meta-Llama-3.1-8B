## Why PEFT

First of all, do you know that when ChatGPT came, it was trained on the GPT-3 model, which had 175 billion parameters? Now, how can you say anyone can fine-tune such a large model without any higher compute resources? 

This is where **Parameter Efficient Fine-Tuning (PEFT)** comes in. In PEFT, we reduce the cost of compute, memory, etc.

**Note:** When LLMs or base models come, they are typically in 32-bit or 16-bit format. So, having such a huge parameter size and later fine-tuning them is a big challenge because we need to convert that `float32` to a lower precision like `int8` to reduce compute and memory usage. Here's how it's done:

## How 32-bit Looks

### What is a Float32 Representation?

A 32-bit floating-point number (`float32`) has the following structure:

- **Sign bit (1 bit):** Indicates positive (0) or negative (1).
- **Exponent (8 bits):** Represents the range of the number.
- **Mantissa (23 bits):** Represents the precision of the number.

Example: Let's say the `float32` number is 5.75. Its binary structure would look like:

- **Sign:** 0 (positive number)
- **Exponent:** 2 (bias = 127 → stored value = 129)
- **Mantissa:** 1.1100... (fractional part in binary)

So, 5.75 (float32) is represented as:

1.110 × 2^2

### Step 2: Quantizing Float32 to Int8

**Goal:** Convert a `float32` model (32 bits per parameter) to an `int8` model (8 bits per parameter) while preserving the model's performance as much as possible.

#### Example: From Float32 to Int8

**Step 1: Starting Float32 Values**

Suppose the model has the following `float32` weights for a layer:

Weights: [4.5, -2.3, 0.0, 5.9, -6.2]

**Step 2: Determine the Float32 Range**

Find the minimum and maximum values in the weights:
Range of Float32 Weights: [-6.2, 5.9]

**Step 3: Decide the Int8 Range**

Int8 is a signed 8-bit integer with the range:

Range of Int8: [-128, 127]

**Step 4: Calculate the Scaling Factor**

The scaling factor is the ratio of the Int8 range to the Float32 range:

Scale Factor = (127 - (-128)) / (5.9 - (-6.2)) ≈ 21.07

**Step 5: Scale the Float32 Values**

Multiply each `float32` weight by the scale factor:

Scaled Value = Float Value × Scale Factor

Let's calculate for each weight:

- `4.5 × 21.07 = 94.815`
- `-2.3 × 21.07 = -48.461`
- `0.0 × 21.07 = 0.0`
- `5.9 × 21.07 = 124.313`
- `-6.2 × 21.07 = -130.634`

**Step 6: Round to the Nearest Integer**

Round the scaled values to the nearest integer:

- `round(94.815) = 95`
- `round(-48.461) = -48`
- `round(0.0) = 0`
- `round(124.313) = 124`
- `round(-130.634) = -131`

**Step 7: Clamp to Int8 Range**

Ensure the rounded values fit into the Int8 range `[-128, 127]`:

- `95` is within range.
- `-48` is within range.
- `0` is within range.
- `124` is within range.
- `-131` is out of range → clamp it to `-128`.

The final `int8` values are:

Quantized Weights: [95, -48, 0, 124, -128]

**Step 8: Store the Scale Factor**

The scale factor `≈ 21.07` is stored as metadata to allow dequantization during inference.

**Step 9: Dequantization (During Inference)**

To reconstruct the `float32` values during inference, multiply the `int8` weights by the inverse of the scale factor:

Float Value = Quantized Value / Scale Factor

Let's dequantize the values:

- `95 / 21.07 ≈ 4.51`
- `-48 / 21.07 ≈ -2.28`
- `0 / 21.07 = 0.0`
- `124 / 21.07 ≈ 5.88`
- `-128 / 21.07 ≈ -6.07`

The dequantized `float32` values are:

[4.51, -2.28, 0.0, 5.88, -6.07]

## Caution

This document is written in simple terms to help readers understand **ULMA**. The content is a mix of original writing and contributions from ChatGPT for better clarity.

