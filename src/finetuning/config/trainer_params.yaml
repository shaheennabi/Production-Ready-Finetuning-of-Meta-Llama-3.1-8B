args:
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  warmup_steps: 20
  max_steps: 300
  learning_rate: 1.5e-4
  logging_steps: 10
  optim: "adamw_8bit"
  weight_decay: 0.02
  lr_scheduler_type: "linear"
  seed: 3407
  output_dir: "outputs"
